{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation With Postgres, Ollama and LangChain\n",
    "\n",
    "The notebook demonstrates how to use the retrieval-augmented generation (RAG) technique to make data stored in Postgres available to an Ollama LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Prerequsites**\n",
    "\n",
    "* Install python and pip\n",
    "* Start a Postgres container with pgvector and upload the movies dataset following the instructions from [chapter8.md](../chapter8.md)\n",
    "* Start an Ollama container and download required embedding and large language models follow the [ai_samples/README.md](README.md) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q psycopg2-binary==2.9.9 langchain-ollama==0.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import psycopg2\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define Function Using Ollama LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    # Connecting to the tinyllama model with the LangChain Ollama interface\n",
    "    llm = OllamaLLM(model=\"tinyllama\", temperature=0.6)\n",
    "\n",
    "    # Generating a final prompt for the LLM based considering the provided context\n",
    "    prompt = f\"\"\"\n",
    "    You're a movie expert and your task is to answer questions about movies based on the provided context.\n",
    "\n",
    "    This is the user's question: {question}  \n",
    "    Consider the following context to provide a detailed and accurate answer: {context}  \n",
    "\n",
    "    The context includes the following details for each movie:\n",
    "    - \"Title\" of the movie\n",
    "    - \"Vote Average\" - the average rating of the movie\n",
    "    - \"Budget\" - the budget allocated for the movie in the US dollars\n",
    "    - \"Revenue\" - the total revenue generated by the movie in the US dollars\n",
    "    - \"Release Date\" - the date the movie was released\n",
    "\n",
    "    Respond in an engaging style that inspires the user to watch the movies.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the LLM passing the prompt. The LLM will generate a response.\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define Function Retreiving Context from Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_postgres(question):\n",
    "    # Connect to the Postgres intance with the pgvector extension\n",
    "    db_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\"\n",
    "    }\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Connect to the embedding model using the OllamaEmbeddings interface\n",
    "    embedding_model = OllamaEmbeddings(model=\"mxbai-embed-large:335m\")\n",
    "\n",
    "    # Generate the embedding for the user's question\n",
    "    embedding = embedding_model.embed_query(question)\n",
    "\n",
    "    # Perform vector similarity search to find relevant movies\n",
    "    query = \"\"\"\n",
    "    SELECT name, vote_average, budget, revenue, release_date\n",
    "    FROM omdb.movies\n",
    "    ORDER BY movie_embedding <=> %s::vector LIMIT 3\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor.execute(query, (embedding, ))\n",
    "    \n",
    "    context = \"\"\n",
    "\n",
    "    # Generate context from the retrieved rows\n",
    "    for row in cursor.fetchall():\n",
    "        context += f\"Movie title: {row[0]}, Vote Average: {row[1]}\"\n",
    "        context += f\", Budget: {row[2]}, Revenue: {row[3]}\"\n",
    "        context += f\", Release Date: {row[4]}\\n\"\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Perform RAG**\n",
    "\n",
    "Using the RAG approach to provide the LLM with more details stored in Postgres. This will augment the LLM's behavior, letting it answer more accurately and with no or fewer hallucinations. You can minimize the hallucinations by tweaking the LLM parameters, the prompt or switching to a more advanced LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a sample question.\n",
    "question = \"I'd like to watch the best movies about pirates.\" + \\\n",
    "    \"Any suggestions?\"\n",
    "\n",
    "# Retrieve context from Postgres based on the user's question\n",
    "context = retrieve_context_from_postgres(question)\n",
    "\n",
    "print(\"Context from Postgres:\")\n",
    "print(context)\n",
    "\n",
    "# Use the context to answer the user's question using the LLM\n",
    "answer = answer_question(question, context)\n",
    "\n",
    "print(\"LLM's answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
