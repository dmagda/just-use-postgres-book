{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation With Postgres, Ollama and LangChain\n",
    "\n",
    "The notebook demonstrates how to use the retrieval-augmented generation (RAG) technique to make data stored in Postgres available to an Ollama LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0. Prerequsites**\n",
    "\n",
    "* Install python and pip\n",
    "* Start a Postgres container with pgvector and upload the movies dataset following the instructions from [chapter8.md](../chapter8.md)\n",
    "* Start an Ollama container and download required embedding and large language models follow the [ai_samples/README.md](README.md) instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install Required Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q psycopg2-binary==2.9.9 langchain-ollama==0.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Define Function Using Ollama LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, context):\n",
    "    llm = OllamaLLM(model=\"tinyllama\", temperature=0.6)\n",
    "\n",
    "    prompt =\"\"\n",
    "    if (context is None) or (context == \"\"):\n",
    "        prompt = f\"Answer the user question: {question}\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Answer the user question: {question} based on the provided context.\n",
    "\n",
    "        The context includes the following details for every movie:\n",
    "        - \"Title\" of the movie\n",
    "        - \"Vote Average\" - the average rating of the movie\n",
    "        - \"Budget\" - the budget allocated for the movie in the US dollars\n",
    "        - \"Revenue\" - the total revenue generated by the movie in the US dollars\n",
    "        - \"Release Date\" - the date the movie was released\n",
    "\n",
    "        Provide a detailed answer to the question, including relevant information from the context.\n",
    "        Add information about the movie plot and characters from your internal knowledge base for every movie mentioned in the context.\n",
    "        The information about each movie must be on a separate line.\n",
    "\n",
    "        Context: \n",
    "        {context}\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Define Function Retreiving Context from Postgres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_postgres(years=5):\n",
    "    db_params = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"password\"\n",
    "    }\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT name, vote_average, budget, revenue, release_date\n",
    "    FROM omdb.movies\n",
    "    WHERE release_date >= NOW() - INTERVAL %s\n",
    "    AND revenue > 0 AND budget > 0 AND vote_average > 0\n",
    "    ORDER BY vote_average DESC, budget ASC\n",
    "    LIMIT 3;\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(query, (f'{years} years',))\n",
    "    \n",
    "    context = \"\"\n",
    "\n",
    "    for row in cursor.fetchall():\n",
    "        context += f\"Movie title: {row[0]}, Vote Average: {row[1]}\"\n",
    "        context += f\", Budget: {row[2]}, Revenue: {row[3]}, Release Date: {row[4]}\\n\"\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Work With LLM Without Providing Context**\n",
    "\n",
    "Requesting the LLM to answer based on its internal knowledge base. It will be hard for the LLM to come up with a proper answer based on the data it was trained on which can lead to hallucinations. Plus, it's highly likely the returned movies will have release years that don't satisfy the search criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 5\n",
    "\n",
    "question = f\"What are the top 3 movies released in the last {years} years with the highest vote average and the lowest budget?\"\n",
    "\n",
    "answer = answer_question(question, None)\n",
    "\n",
    "print(\"LLM's answer without context:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Augment LLM With Context From Postgres**\n",
    "\n",
    "Using the RAG approach to provide the LLM with more details stored in Postgres. This will augment the LLM's behavior, letting it answer more accurately and with no or fewer hallucinations. You can minimize the hallucinations by tweaking the LLM parameters or the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 5\n",
    "\n",
    "question = f\"What are the top 3 movies released in the last {years} years with the highest vote average and the lowest budget?\"\n",
    "\n",
    "context = retrieve_context_from_postgres(years)\n",
    "\n",
    "print(\"Context from Postgres:\")\n",
    "print(context)\n",
    "\n",
    "answer = answer_question(question, context)\n",
    "\n",
    "print(\"LLM's answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
